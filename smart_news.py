# -*- coding: utf-8 -*-
"""smart news.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BS-JkqysOGwR0OxzhUGaNvVoA0Etmaxq
"""

pip install feedparser newspaper3k transformers torch requests

"""
smart_news_curator.py
----------------------------------
An automation tool that:
- Fetches trending news across major sources
- Summarizes and simplifies content
- Formats ready-to-post social updates
"""

import feedparser
from newspaper import Article
from transformers import pipeline
from datetime import datetime
from collections import Counter
import re
import hashlib
import sqlite3
import time

# -------------------------------
# CONFIGURATION
# -------------------------------
RSS_FEEDS = [
    "https://news.google.com/rss?hl=en-IN&gl=IN&ceid=IN:en",
    "https://feeds.bbci.co.uk/news/world/rss.xml",
    "https://rss.nytimes.com/services/xml/rss/nyt/World.xml",
    "https://www.aljazeera.com/xml/rss/all.xml",
    "https://timesofindia.indiatimes.com/rssfeedstopstories.cms"
]

DB_PATH = "news_summaries.db"
SUMMARY_MODEL = "facebook/bart-large-cnn"
MAX_ARTICLES_PER_RUN = 15

# -------------------------------
# DATABASE SETUP
# -------------------------------
conn = sqlite3.connect(DB_PATH)
c = conn.cursor()
c.execute('''
CREATE TABLE IF NOT EXISTS news (
    id TEXT PRIMARY KEY,
    title TEXT,
    url TEXT,
    summary TEXT,
    source TEXT,
    published TEXT,
    created_at TEXT
)
''')
conn.commit()

# -------------------------------
# SUMMARIZER INITIALIZATION
# -------------------------------
print("⏳ Loading summarization model...")
summarizer = pipeline("summarization", model=SUMMARY_MODEL)
print("✅ Model loaded successfully!")

# -------------------------------
# UTILITIES
# -------------------------------
def normalize_title(title):
    """Normalize title for comparison."""
    return re.sub(r'\W+', '', title.lower())

def article_id(url):
    return hashlib.sha256(url.encode("utf-8")).hexdigest()

def already_stored(aid):
    c.execute("SELECT 1 FROM news WHERE id=?", (aid,))
    return c.fetchone() is not None

def fetch_article_text(url):
    """Download and parse article content."""
    try:
        article = Article(url)
        article.download()
        article.parse()
        return article.text, article.title
    except Exception:
        return "", ""

def summarize_text(text):
    """Generate a simplified, clear summary."""
    if not text:
        return ""
    try:
        result = summarizer(text[:1024], max_length=80, min_length=30, do_sample=False)
        return result[0]['summary_text']
    except Exception:
        return text[:250] + "..."

def make_social_post(title, summary, url):
    """Format into a short, readable post."""
    return f"📰 {title}\n\n{summary}\n\nRead more: {url}\n#NewsUpdate #Breaking"

# -------------------------------
# MAIN LOGIC
# -------------------------------
def gather_all_entries():
    print("📡 Gathering news from RSS feeds...")
    all_entries = []
    for feed_url in RSS_FEEDS:
        try:
            feed = feedparser.parse(feed_url)
            for e in feed.entries:
                e["source"] = feed.feed.get("title", "Unknown Source")
                all_entries.append(e)
        except Exception as e:
            print("⚠️ Failed to parse feed:", feed_url, e)
    print(f"✅ Collected {len(all_entries)} raw entries.")
    return all_entries

def get_major_stories(entries):
    """Detect major stories that appear in multiple feeds."""
    counter = Counter(normalize_title(e["title"]) for e in entries)
    top_titles = [t for t, count in counter.items() if count > 1]
    return [e for e in entries if normalize_title(e["title"]) in top_titles]

def process_story(entry):
    url = entry.get("link") or entry.get("id")
    if not url:
        return

    aid = article_id(url)
    if already_stored(aid):
        return

    text, title = fetch_article_text(url)
    if not text:
        return

    summary = summarize_text(text)
    formatted_post = make_social_post(title, summary, url)

    c.execute('''
        INSERT INTO news (id, title, url, summary, source, published, created_at)
        VALUES (?, ?, ?, ?, ?, ?, ?)
    ''', (
        aid,
        title,
        url,
        summary,
        entry.get("source", ""),
        entry.get("published", ""),
        datetime.utcnow().isoformat()
    ))
    conn.commit()

    print("\n✅ New story added:")
    print(formatted_post)
    print("-" * 80)

def main():
    all_entries = gather_all_entries()
    major_stories = get_major_stories(all_entries)

    print(f"🔥 Found {len(major_stories)} major stories.")
    for entry in major_stories[:MAX_ARTICLES_PER_RUN]:
        process_story(entry)
        time.sleep(2)  # polite delay between requests

    print("\n🎉 Run complete! Database updated with new summaries.")

if __name__ == "__main__":
    main()

pip install lxml[html_clean]

"""
smart_news_curator.py
----------------------------------
An automation tool that:
- Fetches trending news across major sources
- Summarizes and simplifies content
- Formats ready-to-post social updates
"""

import feedparser
from newspaper import Article
from transformers import pipeline
from datetime import datetime
from collections import Counter
import re
import hashlib
import sqlite3
import time

# -------------------------------
# CONFIGURATION
# -------------------------------
RSS_FEEDS = [
    "https://news.google.com/rss?hl=en-IN&gl=IN&ceid=IN:en",
    "https://feeds.bbci.co.uk/news/world/rss.xml",
    "https://rss.nytimes.com/services/xml/rss/nyt/World.xml",
    "https://www.aljazeera.com/xml/rss/all.xml",
    "https://timesofindia.indiatimes.com/rssfeedstopstories.cms"
]

DB_PATH = "news_summaries.db"
SUMMARY_MODEL = "facebook/bart-large-cnn"
MAX_ARTICLES_PER_RUN = 15

# -------------------------------
# DATABASE SETUP
# -------------------------------
conn = sqlite3.connect(DB_PATH)
c = conn.cursor()
c.execute('''
CREATE TABLE IF NOT EXISTS news (
    id TEXT PRIMARY KEY,
    title TEXT,
    url TEXT,
    summary TEXT,
    source TEXT,
    published TEXT,
    created_at TEXT
)
''')
conn.commit()

# -------------------------------
# SUMMARIZER INITIALIZATION
# -------------------------------
print("⏳ Loading summarization model...")
summarizer = pipeline("summarization", model=SUMMARY_MODEL)
print("✅ Model loaded successfully!")

# -------------------------------
# UTILITIES
# -------------------------------
def normalize_title(title):
    """Normalize title for comparison."""
    return re.sub(r'\W+', '', title.lower())

def article_id(url):
    return hashlib.sha256(url.encode("utf-8")).hexdigest()

def already_stored(aid):
    c.execute("SELECT 1 FROM news WHERE id=?", (aid,))
    return c.fetchone() is not None

def fetch_article_text(url):
    """Download and parse article content."""
    try:
        article = Article(url)
        article.download()
        article.parse()
        return article.text, article.title
    except Exception:
        return "", ""

def summarize_text(text):
    """Generate a simplified, clear summary."""
    if not text:
        return ""
    try:
        result = summarizer(text[:1024], max_length=80, min_length=30, do_sample=False)
        return result[0]['summary_text']
    except Exception:
        return text[:250] + "..."

def make_social_post(title, summary, url):
    """Format into a short, readable post."""
    return f"📰 {title}\n\n{summary}\n\nRead more: {url}\n#NewsUpdate #Breaking"

# -------------------------------
# MAIN LOGIC
# -------------------------------
def gather_all_entries():
    print("📡 Gathering news from RSS feeds...")
    all_entries = []
    for feed_url in RSS_FEEDS:
        try:
            feed = feedparser.parse(feed_url)
            for e in feed.entries:
                e["source"] = feed.feed.get("title", "Unknown Source")
                all_entries.append(e)
        except Exception as e:
            print("⚠️ Failed to parse feed:", feed_url, e)
    print(f"✅ Collected {len(all_entries)} raw entries.")
    return all_entries

def get_major_stories(entries):
    """Detect major stories that appear in multiple feeds."""
    counter = Counter(normalize_title(e["title"]) for e in entries)
    top_titles = [t for t, count in counter.items() if count > 1]
    return [e for e in entries if normalize_title(e["title"]) in top_titles]

def process_story(entry):
    url = entry.get("link") or entry.get("id")
    if not url:
        return

    aid = article_id(url)
    if already_stored(aid):
        return

    text, title = fetch_article_text(url)
    if not text:
        return

    summary = summarize_text(text)
    formatted_post = make_social_post(title, summary, url)

    c.execute('''
        INSERT INTO news (id, title, url, summary, source, published, created_at)
        VALUES (?, ?, ?, ?, ?, ?, ?)
    ''', (
        aid,
        title,
        url,
        summary,
        entry.get("source", ""),
        entry.get("published", ""),
        datetime.utcnow().isoformat()
    ))
    conn.commit()

    print("\n✅ New story added:")
    print(formatted_post)
    print("-" * 80)

def main():
    all_entries = gather_all_entries()
    major_stories = get_major_stories(all_entries)

    print(f"🔥 Found {len(major_stories)} major stories.")
    for entry in major_stories[:MAX_ARTICLES_PER_RUN]:
        process_story(entry)
        time.sleep(2)  # polite delay between requests

    print("\n🎉 Run complete! Database updated with new summaries.")

if __name__ == "__main__":
    main()